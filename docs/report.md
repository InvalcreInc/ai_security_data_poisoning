# Курсовая работа: Безопасность ИИ - Атаки с отравлением данных в моделях машинного обучения

## 1. Тема: Атаки с отравлением данных в моделях машинного обучения

- **Цель**: Цель данной курсовой работы — проанализировать и продемонстрировать уязвимость моделей машинного обучения к **атакам с отравлением данных**. Эти атаки заключаются в внедрении вредоносных данных в обучающую выборку для того, чтобы повлиять на поведение модели и снизить ее эффективность. В частности, в этой работе рассматриваются **атаки с отравлением данных** как вектор атаки на модель классификации спама, а также способы защиты от таких атак.

## 2. Содержание

### 2.1. Введение

- **Обзор**:  
  Модели машинного обучения широко используются в критически важных приложениях, таких как детекция спама, прогнозирование финансов и диагностика заболеваний. С ростом применения этих моделей увеличивается и интерес злоумышленников к ним. **Атаки с отравлением данных** являются одними из самых опасных видов атак, при которых злоумышленники внедряют искаженные данные в обучающий набор, чтобы нарушить процесс обучения модели.

  В данной курсовой работе используется **датасет Spambase**, популярный набор данных для задачи бинарной классификации (детекция спама), чтобы смоделировать и оценить последствия **атак с отравлением данных**. Мы будем рассматривать **переворачивания меток** (когда спамовые письма неправильно маркируются как не-спам) и **целенаправленные атаки** (когда злоумышленники создают конкретные письма для обхода системы детекции спама). Цель работы — понять влияние таких атак на модель и разработать методы защиты.

- **Исследовательский вопрос**:  
  Как **атаки с отравлением данных** влияют на точность и обобщающую способность моделей машинного обучения, и какие механизмы защиты можно применить для противодействия таким угрозам?

### 2.2. Актуальность

#### 2.2.1. Архитектура и технологии

- **Архитектура**:  
  В этой курсовой работе модель ИИ построена с использованием методов **обучения с учителем**. Основной задачей является обучение модели для классификации писем на спам и не-спам (ham). Архитектура модели включает:

  - **Предобработка данных**:

    - Очистка данных, обработка пропущенных значений, кодирование категориальных признаков и извлечение признаков для подготовки данных к обучению.

  - **Обучение модели**:

    - В качестве базовых моделей используются **логистическая регрессия** и **многочленный наивный байесовский классификатор (MultinomialNB)** для классификации.

  - **Моделирование атаки**:

    - Атаки с **отравлением данных** моделируются путем изменения обучающих данных, например, путём переворачивания меток или внедрения спам-писем, которые будут классифицироваться как не-спам.

  - **Стратегии защиты**:
    - Реализованы несколько методов защиты, таких как **обнаружение выбросов** и **обучение на примерах с атаками** (adversarial training).

- **Технологии**:
  - **Python**: Используется для реализации пайплайна машинного обучения.
  - **Scikit-learn**: Библиотека для обучения моделей машинного обучения, реализации алгоритмов классификации и оценки эффективности моделей.
  - **Matplotlib** и **Seaborn**: Библиотеки для визуализации данных и построения графиков, таких как матрицы ошибок и кривые точности.
  - **Jupyter Notebooks**: Используется для интерактивной работы с данными, выполнения экспериментов и визуализации результатов.

#### 2.2.2. Актуальные темы атак

- **Атака с отравлением данных**:  
  Атаки с отравлением данных могут быть различных типов, но основная идея заключается в том, что злоумышленники манипулируют данными перед обучением модели с целью нарушить поведение модели. В данной работе рассматриваются следующие типы атак:

  - **Переворачивание меток**:  
    Этот метод включает изменение меток в части обучающих данных. Например, спамовые письма могут быть помечены как не-спам (ham), а не-спамовые письма — как спам. Это сбивает модель с толку и делает её менее точной.

  - **Целенаправленная атака**:  
    В этой более сложной атаке злоумышленник создаёт письмо, которое выглядит как не-спам (ham), но на самом деле является спамом. Если это письмо будет введено в обучающую выборку, модель может научиться классифицировать спамовые письма как не-спам, что позволяет злоумышленнику обходить систему.

  - **Влияние отравления данных**:  
    Модель становится смещенной и плохо обобщает на новые данные. Это приводит к снижению точности и ухудшению качества классификации, особенно в критических областях, таких как детекция спама, обнаружение мошенничества или медицинская диагностика.

### 2.3. Проектирование системы

#### 2.3.1. Реализация и тестирование

- **Датасет**:  
  Для задачи бинарной классификации используется **датасет Spambase**. Датасет включает набор email-писем, помеченных как спам (spam) или не-спам (ham). Каждое письмо характеризуется множеством признаков, таких как частоты слов и метаданные.

- **Модель**:  
  Используются две базовые модели машинного обучения:

  - **Логистическая регрессия**: Линейная модель, которая предсказывает вероятность того, что письмо является спамом, исходя из его признаков.

  - **Многочленный наивный байесовский классификатор (MultinomialNB)**: Вероятностная модель, которая предполагает независимость признаков и хорошо работает для задач классификации текста.

- **Реализация атак**:  
  Для моделирования атак с отравлением данных выполняются следующие шаги:

  - **Переворачивание меток**: В случайной части данных метки переводятся на противоположные, например, спамовые письма помечаются как не-спам и наоборот.
  - **Целенаправленная атака**: В базу данных внедряется созданное письмо, которое выглядит как не-спам, но на самом деле является спамом.

- **Методы защиты**:  
  Несколько стратегий защиты от атак с отравлением данных включают:

  - **Обнаружение выбросов**: Применение методов **Principal Component Analysis (PCA)** и **Isolation Forest** для поиска и удаления выбросов (отравленных данных).

  - **Обучение на примерах с атаками**: Обучение модели на смеси чистых и отравленных данных, что делает модель более устойчивой к атакам.

  - **Регуляризация**: Применение **L2 регуляризации** для предотвращения переобучения на отравленных данных, что улучшает обобщение модели.

- **Тестирование**:  
  Оценка производительности модели проводится с использованием метрик:

  - **Точность (Accuracy)**: Измеряется доля правильно классифицированных писем.
  - **Матрица ошибок (Confusion Matrix)**: Оценка ложных срабатываний, ложных отрицаний, точности, полноты и F1-метрики.
  - **F1-Оценка**: Сбалансированная метрика точности и полноты.

#### 2.3.2. Анализ результатов

- **Метрики производительности**:
  Результаты показывают значительное снижение производительности модели при добавлении отравленных данных. Например, **точность** и **F1-оценка** снижаются, и модель становится менее надежной при классификации спам-писем.

  - **До атаки**: Модель имеет высокую точность в классификации спама и не-спама.
  - **После атаки**: Точность снижается, и модель начинает ошибочно классифицировать письма. Матрица ошибок показывает увеличение ложных срабатываний (не-спамовые письма, классифицированные как спам) и ложных отрицаний (спамовые письма, классифицированные как не-спам).

- **Эффективность методов защиты**:
  - **Обнаружение выбросов**: Удаление выбросов (отравленных данных) значительно улучшает точность модели и снижает количество ошибок.
  - **Обучение на примерах с атаками**: Обучение модели как на чистых, так и на отравленных данных повышает ее устойчивость к атакам.
  - **Регуляризация**: Применение регуляризации помогает избежать переобучения и повышает способность модели обобщать.

### 2.4. Заключение

- **Резюме выводов**:  
  Атаки с отравлением данных значительно снижают производительность моделей машинного обучения, и простые меры защиты, такие как **обнаружение выбросов** и **обучение на примерах с атаками**, могут эффективно смягчить эти угрозы. Модели становятся более устойчивыми к атакам, если они обучаются как на чистых, так и на отравленных данных.

- **Влияние на безопасность ИИ**:  
  Эта работа подчеркивает важность **защиты моделей машинного обучения** и необходимость включения механизмов защиты на всех этапах разработки модели. Особенно это важно для приложений в критических областях, таких как финансы, здравоохранение и системы безопасности.

- **Будущая работа**:  
  В дальнейшем можно изучить более сложные атаки, такие как **атаки с использованием скрытых каналов** (backdoor attacks) и методы **проверки целостности моделей**, которые позволят повысить безопасность моделей машинного обучения.
